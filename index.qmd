---
title: "Report Sample"
author: "Student name"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to Kernel regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].

## Article Summary

Article 1: On beta regression residuals

Patrícia L. Espinheira, Silvia L.P. Ferrari & Francisco
Cribari-Neto (2008) On beta regression residuals, Journal of Applied
Statistics, 35:4, 407-419, DOI: 10.1080/0266476070183493

Summary

The article discusses the application of beta regression models in
statistical analysis, specifically focusing on scenarios where
continuous variables are bounded within the open interval (0, 1). The
primary goal of the paper is to address the challenge of analyzing data
involving continuous variables that are confined to the open interval
(0, 1), such as proportions and percentages. Traditional linear
regression models are not suited for such data due to their inability to
account for the specific characteristics of this type of information,
including limited range and varying variances. The paper introduces beta
regression models as an alternative and proposes innovative residuals
for assessing model performance and identifying influential data points.

This research is significant because it offers a practical solution to a
common statistical problem encountered in diverse fields. By introducing
these new residuals based on Fisher's scoring iterative algorithm, the
paper presents a methodological advancement that enhances the accuracy
of model assessments and identification of influential observations. The
results of Monte Carlo simulations and real data applications
demonstrate the superiority of these new residuals, especially in
scenarios where traditional residuals may fall short in assessing model
fit and identifying influential data points.

Article 2: Improved estimators for a general class of beta regression
models

Simas, Alexandre B. & Barreto-Souza, Wagner & Rocha, Andréa V., 2010.
"Improved estimators for a general class of beta regression
models," Computational Statistics & Data Analysis, Elsevier, vol. 54(2),
pages 348-366, February.

Summary

The goal of this paper is to enhance the beta regression model by
allowing for nonlinear regression structures and precision parameter
regression. The importance of this work lies in addressing biases in
parameter estimation that can arise, particularly in small sample sizes,
and in providing practical solutions for bias correction. This is
crucial because biased estimates can lead to erroneous conclusions in
statistical analyses.

To achieve this goal, the paper derives closed-form expressions for
second-order biases in maximum likelihood estimators (MLEs) of various
model parameters, including means of responses and precision parameters.
The methods for bias correction explored in the paper include analytical
approaches, preventive bias reduction, bootstrap bias adjustment, and
auxiliary weighted linear regressions. The paper presents simulation
results showing that the proposed bias-corrected estimators outperform
the MLEs, particularly in small sample sizes. However, this approach
might have limitations or assumptions that should be considered when
applying it to specific datasets or scenarios, and further research may
be needed to explore its applicability in different contexts.

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

## Analysis and Results

### Data and Vizualisation

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

### Conlusion

## References
